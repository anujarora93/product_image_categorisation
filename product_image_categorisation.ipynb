{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2uBM40twm3gn"
   },
   "source": [
    "# Fashion product categorisation and attribute extraction from images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5zVkX1p7mwVZ"
   },
   "source": [
    "### Paper: \n",
    "\n",
    "**A Unified Model with Structured Output for Fashion Images Classification - Ferreira *et al* (2018)**\n",
    "\n",
    "**Abstract**: A picture is worth a thousand words. Albeit a cliché, for the fashion industry, an image of a clothing piece allows one to perceive its category (e.g., dress), sub-category (e.g., day dress) and properties (e.g., white colour with floral patterns). The seasonal nature of the fashion industry creates a highly dynamic and creative domain with evermore data, making it unpractical to manually describe a large set of images (of products). In this paper, we explore the concept of visual recognition for fashion images through an end-to-end architecture embedding the hierarchical nature of the annotations directly into the model. Towards that goal, and inspired by the work of [7], we have modified and adapted the original architecture proposal. Namely, we have removed the message passing layer symmetry to cope with Farfetch category tree, added extra layers for hierarchy level specificity, and moved the message passing layer into an enriched latent space. We compare the proposed unified architecture against state-of-the-art models and demonstrate the performance advantage of our model for structured multi-level categorization on a dataset of about 350k fashion product images.\n",
    "\n",
    "\n",
    "https://arxiv.org/abs/1806.09445\n",
    "\n",
    "\n",
    "Throughout this notebook, I have interspersed passages from the paper which describe the architecture, training strategy, etc.\n",
    "\n",
    "### Tensorboard\n",
    "\n",
    "If using TensorFlow 2.0, it is possible to run TensorBoard inline within this notebook.\n",
    "\n",
    "However, the model architecture defined below will work best with tf v1.14. In this case, TensorBoard can still be run inline using the `tensorboardcolab` package, or in a separate tf2.0 notebook pointed to the correct log directory.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ckgNFEBHy_1F"
   },
   "source": [
    "## Imports etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAFFNOshCOym"
   },
   "outputs": [],
   "source": [
    "# Specify whether to use TF2.0 (which enables inline tensorboard)\n",
    "use_tf_2 = False\n",
    "\n",
    "\n",
    "# Specify whether to use the \"small\" or \"large\" images\n",
    "IMAGE_SIZE = \"large\"\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "target_num_samples = 25000\n",
    "\n",
    "TRAIN_SAMPLE = 0.85\n",
    "\n",
    "AUGMENT_FLAG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:48.290278Z",
     "start_time": "2019-05-04T19:04:46.274255Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Ls5mX_ioy_1H",
    "outputId": "9bdd84ee-b425-41a2-e94c-ee00827eeae5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import datetime\n",
    "# from PIL import Image\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import cv2\n",
    "from albumentations import (\n",
    "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
    "    RandomBrightness, RandomContrast, RandomGamma,\n",
    "    ToFloat, ShiftScaleRotate)\n",
    "\n",
    "if use_tf_2:\n",
    "  from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "  from tensorflow.keras.preprocessing import image\n",
    "  from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "  from tensorflow.keras.models import Sequential, Model, load_model\n",
    "  from tensorflow.keras.layers import Input, Dense, Add, Dropout\n",
    "  from tensorflow.keras.activations import softmax, sigmoid\n",
    "  from tensorflow.keras import regularizers\n",
    "  from tensorflow.keras.optimizers import Adam as adam\n",
    "  from tensorflow.keras.losses import categorical_crossentropy, binary_crossentropy\n",
    "  from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "  from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "  from tensorflow.keras.utils import to_categorical, Sequence\n",
    "  from sklearn.preprocessing import MultiLabelBinarizer\n",
    "  \n",
    "else:\n",
    "  from keras.applications.resnet50 import ResNet50\n",
    "  from keras.preprocessing import image\n",
    "  from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "  from keras.models import Sequential, Model, load_model\n",
    "  from keras.layers import Input, Dense, Add, Dropout\n",
    "  from keras.activations import softmax, sigmoid\n",
    "  from keras import regularizers\n",
    "  from keras.optimizers import adam\n",
    "  from keras.losses import categorical_crossentropy, binary_crossentropy\n",
    "  from keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "  from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "  from keras.utils import to_categorical, Sequence\n",
    "  from sklearn.preprocessing import MultiLabelBinarizer\n",
    "  import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V1WjFdsgj0Xt"
   },
   "outputs": [],
   "source": [
    "if use_tf_2:\n",
    "  !pip install -q tf-nightly-2.0-preview\n",
    "  # # Load the TensorBoard notebook extension\n",
    "  %load_ext tensorboard\n",
    "  \n",
    "# else:\n",
    "#   !pip install tensorboardcolab\n",
    "#   from tensorboardcolab import TensorBoardColab, TensorBoardColabCallback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFPt2mKWW-zq"
   },
   "outputs": [],
   "source": [
    "today = datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "\n",
    "OUTPUT_DIR = f'./outputs_{IMAGE_SIZE}/{today}/'\n",
    "if not os.path.exists(OUTPUT_DIR + 'models'):\n",
    "  os.makedirs(OUTPUT_DIR + 'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6sltBxIy_1W"
   },
   "source": [
    "## Data ingestion and prep\n",
    "\n",
    "**Potential datasets:**\n",
    "\n",
    "Clothing attributes dataset: https://exhibits.stanford.edu/data/catalog/tb980qz1002\n",
    "\n",
    "Fashionista: https://github.com/grahamar/fashion_dataset/tree/master/fashionista\n",
    "\n",
    "Fashion mnist: https://github.com/zalandoresearch/fashion-mnist\n",
    "\n",
    "Deepfashion: http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html, http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/AttributePrediction.html\n",
    "\n",
    "\n",
    "Product attributes: https://rloganiv.github.io/mae/\n",
    "\n",
    "**Dataset used below:**\n",
    "\n",
    "Product cats (with hierarchy) and attrs: https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8BEaNoRzibu",
    "outputId": "25437836-b13c-44c1-8390-f559fa3d127b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "B6kwQkis9nY9",
    "outputId": "6a751b36-90a5-4b0d-a98c-d5dfec43a3f0"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir('gdrive/My Drive/Colab Notebooks/fashion_images')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GqStAPAVotuM"
   },
   "source": [
    "Here we will assume you are working in Google Colab, with the top level directory being: \n",
    "\n",
    "`gdrive/My Drive/Colab Notebooks/fashion_images`\n",
    "\n",
    "Within this top level directory, download and unzip the product images dataset (and associated labels csv) to `fashion-product-images-small` or `fashion-product-images-large` depending on the dataset used.\n",
    "\n",
    "Google Drive has a [known issue](https://github.com/googlecolab/colabtools/issues/382) whereby it will repeatedly time out if a folder contains many thousands of files - the fashion images dataset contains circa 50,000 images.\n",
    "\n",
    "In order to mitigate this, this repository includes a shell script which can be run from the `move_images.ipynb` notebook to batch the images into subdirectories of ~1500 files each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a0QzmJZg0UAr"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.upload()  #this will prompt you to upload the kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C7miPJOL0-KQ"
   },
   "outputs": [],
   "source": [
    "# !pip install -q kaggle\n",
    "# !mkdir -p ~/.kaggle\n",
    "# !cp kaggle.json ~/.kaggle/\n",
    "# !ls ~/.kaggle\n",
    "# !chmod 600 /root/.kaggle/kaggle.json  # set permission\n",
    "\n",
    "# if IMAGE_SIZE == 'small'\n",
    "#   ! kaggle datasets download paramaggarwal/fashion-product-images-small -p /content/gdrive/My\\ Drive/Colab\\ Notebooks/fashion_images/fashion-dataset-small/\n",
    "#   ! unzip fashion-dataset-small/fashion-product-images-small.zip\n",
    "\n",
    "# else:\n",
    "#   # Large version\n",
    "#   ! kaggle datasets download paramaggarwal/fashion-product-images-dataset -p /content/gdrive/My\\ Drive/Colab\\ Notebooks/fashion_images/fashion-dataset-large-2/\n",
    "#   ! unzip fashion-dataset-large/fashion-product-images-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:48.413931Z",
     "start_time": "2019-05-04T19:04:48.301089Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "qfubsZ3Sy_1Z"
   },
   "outputs": [],
   "source": [
    "raw_data_df = pd.read_csv(f'./fashion-dataset-{IMAGE_SIZE}/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:48.439617Z",
     "start_time": "2019-05-04T19:04:48.416900Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "IgVWLZjly_1c",
    "outputId": "20f66742-ff0b-4afc-91d2-f27cc1e00a38"
   },
   "outputs": [],
   "source": [
    "raw_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:48.613455Z",
     "start_time": "2019-05-04T19:04:48.443376Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "xH5raV03y_1h",
    "outputId": "83eccd39-1f58-4a29-fd93-681658a8ed75"
   },
   "outputs": [],
   "source": [
    "labels_df = raw_data_df[['id', 'subCategory', 'articleType', 'season', 'usage']]\n",
    "labels_df['attributes'] = labels_df[['season','usage']].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "# Get the filepaths for all the images in their subfolders\n",
    "image_base_dir = f'./fashion-dataset-{IMAGE_SIZE}/images/'\n",
    "\n",
    "image_fnames = [f for f in glob.glob(image_base_dir + \"**/*.jpg\", recursive=True)]\n",
    "\n",
    "image_ids = [int(f.split('/')[-1][:-4]) for f in image_fnames]\n",
    "\n",
    "\n",
    "\n",
    "image_ids_df = pd.DataFrame({'id':image_ids,'fname':image_fnames}, index=None)\n",
    "\n",
    "\n",
    "\n",
    "# Inner join to make sure we have both the image and its labels\n",
    "labels_df = labels_df.merge(image_ids_df, how='inner', on='id')\n",
    "\n",
    "labels_df.reset_index(drop='index', inplace=True)\n",
    "\n",
    "\n",
    "# Optionally take a random subset of the data to speed up prototyping\n",
    "if target_num_samples is not None:\n",
    "  \n",
    "  # Limit the classes so that no 1 class has > 5 x the mean class size\n",
    "  max_category_num = int(4 * np.mean(labels_df.subCategory.value_counts().values))\n",
    "  labels_df = labels_df.groupby('subCategory', group_keys=False).apply(lambda x: x.sample(min(len(x), max_category_num),\n",
    "                                                                                         random_state=42))\n",
    "  \n",
    "  # Calculate what fraction of the total dataset we need to get the target num of samples\n",
    "  # Also ensure this doesn't go above 1\n",
    "  downsample_proportion = min(1,target_num_samples/labels_df.shape[0])\n",
    "  \n",
    "  # Perform the downsampling\n",
    "  labels_df = labels_df.groupby('subCategory', group_keys=False).apply(lambda x: x.sample(int(len(x)*downsample_proportion),\n",
    "                                                                                         random_state=42))\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "#   Shuffle the dataframe\n",
    "labels_df = labels_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "\n",
    "  \n",
    "# Recover the list of image filenames from the dataframe, to ensure orders match\n",
    "image_fnames = list(labels_df['fname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kNd9cSGAhjg8"
   },
   "outputs": [],
   "source": [
    "#  define data augmentation\n",
    "\n",
    "if AUGMENT_FLAG:\n",
    "  AUGMENTATIONS_TRAIN = Compose([\n",
    "      HorizontalFlip(p=0.5),\n",
    "      RandomContrast(limit=0.2, p=0.5),\n",
    "      RandomBrightness(limit=0.2, p=0.5),\n",
    "      ShiftScaleRotate(\n",
    "          shift_limit=0.0625, scale_limit=0.1, \n",
    "          rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), \n",
    "      ToFloat(max_value=255)\n",
    "  ])\n",
    "\n",
    "  AUGMENTATIONS_TEST = Compose([\n",
    "      ToFloat(max_value=255)\n",
    "  ])\n",
    "  \n",
    "else:\n",
    "  AUGMENTATIONS_TRAIN = None\n",
    "  AUGMENTATIONS_TEST = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z0T8GtZknMFg"
   },
   "source": [
    "## Create train/test split, and sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5u5Jrp0MiEa"
   },
   "outputs": [],
   "source": [
    "# Here, `x_set` is list of path to the images\n",
    "# and `y_set` are the associated classes.\n",
    "\n",
    "class FashionSequence(Sequence):\n",
    "\n",
    "    def __init__(self, x_set, y_cat, y_subcat, y_attr, batch_size, augmentations=None):\n",
    "        self.x = x_set\n",
    "        self.y_cat = y_cat\n",
    "        self.y_subcat = y_subcat\n",
    "        self.y_attr = y_attr\n",
    "        self.batch_size = batch_size\n",
    "        self.augment = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y_cat = self.y_cat[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y_subcat = self.y_subcat[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y_attr = self.y_attr[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        if self.augment == None:\n",
    "          return np.array([\n",
    "              resize(imread(file_name), (224, 224,3))\n",
    "                 for file_name in batch_x]), [batch_y_cat, batch_y_subcat, batch_y_attr]          \n",
    "        \n",
    "        else:\n",
    "          return np.array([\n",
    "            self.augment(image=resize(imread(file_name), (224, 224,3)))[\"image\"]\n",
    "            for file_name in batch_x]), [batch_y_cat, batch_y_subcat, batch_y_attr]\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:56.712949Z",
     "start_time": "2019-05-04T19:04:56.710440Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tp2BWhiwy_1r",
    "outputId": "5156e1a5-fcb3-486a-c5a6-dc356d322617"
   },
   "outputs": [],
   "source": [
    "num_samples = len(image_fnames)\n",
    "print(f\"{num_samples} samples successfully loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:56.719477Z",
     "start_time": "2019-05-04T19:04:56.715077Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "F6e2bKrSy_1t",
    "outputId": "a9388647-3cda-4146-8669-17054646221e"
   },
   "outputs": [],
   "source": [
    "num_samples_train = int(num_samples * TRAIN_SAMPLE)\n",
    "num_samples_test = num_samples - num_samples_train\n",
    "\n",
    "print(f\"{num_samples_train} training and {num_samples_test} testing samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:56.829802Z",
     "start_time": "2019-05-04T19:04:56.721533Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "jungIrJgy_1x"
   },
   "outputs": [],
   "source": [
    "# Get category labels\n",
    "cat_names = pd.Categorical(labels_df.subCategory).categories\n",
    "cat_labels_all = to_categorical(pd.Categorical(labels_df.subCategory).codes)\n",
    "num_categories = cat_labels_all.shape[1]\n",
    "\n",
    "\n",
    "# Get subcategory labels\n",
    "subcat_names = pd.Categorical(labels_df.articleType).categories\n",
    "subcat_labels_all = to_categorical(pd.Categorical(labels_df.articleType).codes)\n",
    "num_subcats = subcat_labels_all.shape[1]\n",
    "\n",
    "\n",
    "# Get attribute labels\n",
    "all_attributes = labels_df.attributes.values\n",
    "attributes_set = list(set(list(labels_df.season) + list(labels_df.usage)))\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=attributes_set)\n",
    "mlb.fit([tuple(f) for f in all_attributes])\n",
    "attribute_labels_all = mlb.transform(all_attributes)\n",
    "\n",
    "attribute_names = mlb.classes_\n",
    "num_attributes = attribute_labels_all.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:56.834522Z",
     "start_time": "2019-05-04T19:04:56.831474Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "itLAcMUFy_1z",
    "outputId": "054da35d-fc83-45de-f6b5-92fd92a46ba1"
   },
   "outputs": [],
   "source": [
    "print(f\"{num_categories} categories and {num_subcats} subcategories\")\n",
    "\n",
    "print(f\"{num_attributes} possible attributes\")\n",
    "\n",
    "print(f\"Possible attributes: {mlb.classes_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2DAL17l4mA7"
   },
   "outputs": [],
   "source": [
    "#  Get the train and test sample indices, stratified by category\n",
    "\n",
    "train_indices = sorted(list(labels_df.groupby('subCategory', group_keys=False).apply(lambda x: x.sample(int(len(x)*TRAIN_SAMPLE), random_state=42)).index))\n",
    "\n",
    "test_indices = [x for x in list(labels_df.index) if x not in train_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:04:56.841346Z",
     "start_time": "2019-05-04T19:04:56.836550Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "h17zp97By_12"
   },
   "outputs": [],
   "source": [
    "cat_labels_train = cat_labels_all[train_indices]\n",
    "subcat_labels_train = subcat_labels_all[train_indices]\n",
    "attribute_labels_train = attribute_labels_all[train_indices]\n",
    "\n",
    "\n",
    "cat_labels_test = cat_labels_all[test_indices]\n",
    "subcat_labels_test = subcat_labels_all[test_indices]\n",
    "attribute_labels_test = attribute_labels_all[test_indices]\n",
    "\n",
    "img_list_train = list(np.array(image_fnames)[train_indices])\n",
    "img_list_test = list(np.array(image_fnames)[test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BPlxDiRDT0I7"
   },
   "outputs": [],
   "source": [
    "train_generator = FashionSequence(x_set = img_list_train,\n",
    "                                  y_cat = cat_labels_train,\n",
    "                                  y_subcat = subcat_labels_train,\n",
    "                                  y_attr = attribute_labels_train, \n",
    "                                  batch_size = batch_size,\n",
    "                                  augmentations = AUGMENTATIONS_TRAIN)\n",
    "\n",
    "test_generator = FashionSequence(x_set = img_list_test,\n",
    "                                  y_cat = cat_labels_test,\n",
    "                                  y_subcat = subcat_labels_test,\n",
    "                                  y_attr = attribute_labels_test, \n",
    "                                  batch_size = batch_size,\n",
    "                                  augmentations = AUGMENTATIONS_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aw5V95Tzy_13"
   },
   "source": [
    "## Define model architecture:\n",
    "\n",
    "\n",
    "The message passing block (that encodes the category tree) is built on top of the off-the- shelf convolutional neural network ResNet-50 [5], pre-trained (i.e., with weights initialised as the weights learned after training the network) on the ImageNet [4].\n",
    "\n",
    "\n",
    "Additionally, three parallel dense layers (one per hierarchy level) of dimension 1024 are connected to theoutputoftheResNet-50. ThesewillbetheinputsoftheMessage Propagation block.\n",
    "\n",
    "\n",
    "Every Dense layer defined in the Message Propagation block is of dimension 1024 with a L2-norm regularization and regularization factor of 0.0005 (promoting the learning of more uniform weights, thus reducing the risk of over-fitting) followed by ReLU activation layers. \n",
    "\n",
    "\n",
    "The final Dense layers of this block (the Dense layers shown in Figure 7) are also followed by a Dropout [6] of rate 0.3.\n",
    "\n",
    "The full architecture (encompassing the ResNet-50, intermediate dense layers for each level and the message passing block) totals 46.915.690 trainable parameters.\n",
    "\n",
    "Output activations for each level predictions depend on the problem at hand, i.e., as the category and sub-category level predictions are multi-class problems we use a softmax function as activation, while at the attribute level we have a multi-label problem and thus we use a sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:05:01.933933Z",
     "start_time": "2019-05-04T19:04:56.844520Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "d4ej8zMhy_14",
    "outputId": "8127b7a7-c8da-4c9a-c28b-686e80d618de"
   },
   "outputs": [],
   "source": [
    "base_model = ResNet50(weights='imagenet')\n",
    "\n",
    "\n",
    "x_cat = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(base_model.output)\n",
    "x_subcat = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(base_model.output)\n",
    "x_attr = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(base_model.output)\n",
    "\n",
    "\n",
    "### Forward pass (left hand side)\n",
    "dense_1 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x_cat)\n",
    "dense_cat_b= Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(dense_1)\n",
    "dense_cat_b = Dropout(0.3)(dense_cat_b)\n",
    "\n",
    "dense_2 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x_subcat)\n",
    "dense_3 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(dense_cat_b)\n",
    "add_1 = Add()([dense_2, dense_3])\n",
    "dense_subcat_b = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(add_1)\n",
    "dense_subcat_b = Dropout(0.3)(dense_subcat_b)\n",
    "\n",
    "dense_4 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x_attr)\n",
    "dense_5 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(dense_cat_b)\n",
    "add_2 = Add()([dense_4, dense_5])\n",
    "dense_attr_b = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(add_2)\n",
    "dense_attr_b = Dropout(0.3)(dense_attr_b)\n",
    "\n",
    "\n",
    "### Backward pass (right hand side)\n",
    "dense_6 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x_attr)\n",
    "dense_attr_g = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(dense_6)\n",
    "dense_attr_g = Dropout(0.3)(dense_attr_g)\n",
    "\n",
    "dense_7 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x_subcat)\n",
    "dense_subcat_g = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(dense_7)\n",
    "dense_subcat_g = Dropout(0.3)(dense_subcat_g)\n",
    "\n",
    "dense_8 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(x_cat)\n",
    "dense_9 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(dense_subcat_g)\n",
    "dense_10 = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(dense_attr_g)\n",
    "add_3 = Add()([dense_9, dense_10])\n",
    "add_4 = Add()([dense_8, add_3])\n",
    "dense_cat_g = Dense(units=1024, activation='relu', kernel_regularizer=regularizers.l2(0.0005))(add_4)\n",
    "dense_cat_g = Dropout(0.3)(dense_cat_g)\n",
    "\n",
    "\n",
    "add_cat = Add()([dense_cat_b, dense_cat_g])\n",
    "cat_out = Dense(units=num_categories, activation='softmax', kernel_regularizer=regularizers.l2(0.0005), name=\"Category_output\")(add_cat)\n",
    "\n",
    "add_subcat = Add()([dense_subcat_b, dense_subcat_g])\n",
    "subcat_out = Dense(units=num_subcats, activation='softmax', kernel_regularizer=regularizers.l2(0.0005), name=\"Subcategory_output\")(add_subcat)\n",
    "\n",
    "add_attr = Add()([dense_attr_b, dense_attr_g])\n",
    "attr_out = Dense(units=num_attributes, activation='sigmoid', kernel_regularizer=regularizers.l2(0.0005), name=\"Attributes_output\")(add_attr)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:05:02.326692Z",
     "start_time": "2019-05-04T19:05:02.307677Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "o7F4P9vqy_1-"
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=base_model.input, outputs=[cat_out, subcat_out, attr_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLzsl3Jey_1_"
   },
   "source": [
    "## Training strategy and evaluation metrics\n",
    "\n",
    "The network is trained by minimising a weighted cross-entropy loss for each level in order to estimate the parameters that originate the most correct predictions for the category, sub-category and attribute levels.\n",
    "\n",
    "A weighting mechanism is used to address class imbalance, a common issue that also arises in our dataset. In partic- ular, we compute the occurrence frequency of each class/label and apply a customised cross-entropy loss where the penalisation is weighted by the inverse of its frequency. \n",
    "\n",
    "Hence, the loss for predict- ing more frequent classes is down-weighted while when predicting more rare classes the loss is penalised. This way, all classes per level should be equally important during the training process of the model. \n",
    "\n",
    "Also, contrarily to what is presented in [7], we train our model in a single-shot fashion (end-to-end). \n",
    "\n",
    "The loss functions are optimised via backpropagation and batched-based Adam [8], with a batch size of 32 images and a learning rate of 0.001\n",
    "\n",
    "\n",
    "**Evaluation Metrics**: For category and sub-category classifica- tion we choose the class with highest estimated confidence score. \n",
    "\n",
    "For the multi-label attribute classification, the labels are predicted as positive if the predicted label confidence is greater than 0.75. This threshold was chosen in conjunction with the business (to find a good ratio between adding new attributes without making serious mistakes). Nevertheless, we will present some metrics that are threshold independent to allow a better comparison between each approach.\n",
    "\n",
    "For the multi-class problems, for category and sub-category levels, we report overall precision (OP), recall (OR), and F1-score (OF1), weighted by class support, i.e., the number of true instances for each class.\n",
    "\n",
    "Therefore, given that we are using a weighted version of the macro precision and recall, the resulting F1-scores may not be between precision and recall.\n",
    "\n",
    "For the multi-label classification, at attributes level, we also em- ploy overall precision (OP), recall (OR), and F1-score (OF1) for performance comparison. Moreover, we use precision (P@k), recall (R@k) and F1-score (F1@k) @ top K labels (where K is the number of ground truth labels that each product is annotated with). \n",
    "\n",
    "We also report the average precision (AP), which summarises the precision- recall curve. The previous metrics allow us to assess the method performance irrespective of defining a threshold on the confidence scores for positive/negative classification. For all these metrics, the larger value, the better the performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:05:02.348241Z",
     "start_time": "2019-05-04T19:05:02.328580Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ajoyYiDyy_1_"
   },
   "outputs": [],
   "source": [
    "opt = adam(lr=0.001)\n",
    "\n",
    "\n",
    "losses = {'Category_output':'categorical_crossentropy',\n",
    "         'Subcategory_output':'categorical_crossentropy',\n",
    "         'Attributes_output':'binary_crossentropy'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:05:02.354261Z",
     "start_time": "2019-05-04T19:05:02.349876Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "kkaHwbIuy_2B"
   },
   "outputs": [],
   "source": [
    "metrics = {\"Category_output\":\"categorical_accuracy\",\n",
    "          \"Subcategory_output\":\"categorical_accuracy\",\n",
    "          \"Attributes_output\":\"top_k_categorical_accuracy\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iIFZg73oBx4"
   },
   "source": [
    "## Model compilation and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:05:02.461020Z",
     "start_time": "2019-05-04T19:05:02.356604Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "C0FZG3Kpy_2C",
    "outputId": "ceb0f478-178d-4513-c977-f0927f84f06b"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=losses, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:05:02.501250Z",
     "start_time": "2019-05-04T19:05:02.462876Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "T0xJ2xnBy_2F",
    "outputId": "7410707c-30dc-4077-8584-30c084767c85"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:05:02.511399Z",
     "start_time": "2019-05-04T19:05:02.503067Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "1VJ6osury_2H"
   },
   "outputs": [],
   "source": [
    "cat_weights = dict([(k,v) for k,v in zip(np.arange(num_categories),1/np.sum(cat_labels_train+0.001, axis=0))])\n",
    "subcat_weights = dict([(k,v) for k,v in zip(np.arange(num_subcats),1/np.sum(subcat_labels_train+0.001, axis=0))])\n",
    "attr_weights = dict([(k,v) for k,v in zip(np.arange(num_attributes),1/np.sum(attribute_labels_train+0.001, axis=0))])\n",
    "\n",
    "weights_dict = {'Category_output':cat_weights,\n",
    "               'Subcategory_output':subcat_weights,\n",
    "                  'Attributes_output':attr_weights}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mmYlPqR6iR2O"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = ModelCheckpoint(filepath=OUTPUT_DIR + 'models/model-e-{epoch:02d}-loss-{val_loss:.2f}.hdf5', monitor='val_loss')\n",
    "\n",
    "\n",
    "# if use_tf_2:\n",
    "tb_callback = TensorBoard(log_dir=f'{OUTPUT_DIR}/logs', \n",
    "                          histogram_freq=0, \n",
    "                          batch_size=batch_size, \n",
    "                          update_freq=2048,\n",
    "                         write_grads=True,\n",
    "                          write_images=True)\n",
    "\n",
    "# else:\n",
    "#   tbc=TensorBoardColab()\n",
    "#   tb_callback = TensorBoardColabCallback(tbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mF9ixuo-naJs"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jz0yYGV-kt1T"
   },
   "outputs": [],
   "source": [
    "if use_tf_2:\n",
    "  %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:57:54.234074Z",
     "start_time": "2019-05-04T19:05:02.513586Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "uq2GKbmWy_2J",
    "outputId": "93e85adf-f199-4ff5-bcc4-6586740171a4"
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator, \n",
    "                    steps_per_epoch=None, \n",
    "                    epochs=25, \n",
    "                    verbose=1, \n",
    "                    callbacks=[checkpoint_callback, tb_callback],\n",
    "                    validation_data=test_generator, \n",
    "                    validation_steps=None, \n",
    "                    class_weight=weights_dict, \n",
    "                    max_queue_size=64, \n",
    "                    workers=8, \n",
    "                    use_multiprocessing=True, \n",
    "                    shuffle=True,\n",
    "                    initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "woZb34sHJeWY",
    "outputId": "49a33d4b-925c-4756-bb26-0e8ef7d7e019"
   },
   "outputs": [],
   "source": [
    "for k in model.history.history.keys():\n",
    "    print (f\"Final {k}:  {model.history.history[k][-1]}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqEX2HyNndAl"
   },
   "source": [
    "## Predicting with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:58:33.727529Z",
     "start_time": "2019-05-04T19:57:54.238922Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ijYP-4xCy_2L"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T19:59:23.140027Z",
     "start_time": "2019-05-04T19:58:42.893994Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "c-ltq41Gy_2N",
    "outputId": "cf455013-121a-4202-c67f-ea7e358b1feb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(250):\n",
    "    print(f\"Input image {i}:\")\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.imshow(plt.imread(img_list_test[i]))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    cat_pred = cat_names[np.argmax(predictions[0][i])]\n",
    "    cat_pred_score = str(max(predictions[0][i]))[:5]\n",
    "    \n",
    "    subcat_pred = subcat_names[np.argmax(predictions[1][i])]\n",
    "    subcat_pred_score = str(max(predictions[1][i]))[:5]\n",
    "    \n",
    "    attr_preds = attribute_names[np.where(predictions[2][i] > 0.7)]\n",
    "    attr_pred_scores = np.around(predictions[2][i][np.where(predictions[2][i] > 0.7)],3)\n",
    "    \n",
    "    gt_cat = cat_names[np.where(cat_labels_test[i])][0]\n",
    "    gt_subcat = subcat_names[np.where(subcat_labels_test[i])][0]\n",
    "    gt_attr = attribute_names[np.where(attribute_labels_test[i])]\n",
    "\n",
    "    print(f\"Actual category:       {gt_cat}\")\n",
    "    print(f\"Predicted category:    {cat_pred} ({cat_pred_score})\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    print(f\"Actual subcategory:    {gt_subcat}\")\n",
    "    print(f\"Predicted subcategory: {subcat_pred} ({subcat_pred_score})\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "    print(\"Actual attributes:    \", \", \".join(gt_attr))\n",
    "    print(\"Predicted attributes: \", \", \".join(\"{} ({})\".format(x, str(y)[:5]) for x, y in zip(attr_preds, attr_pred_scores)))\n",
    "\n",
    "    \n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ckgNFEBHy_1F",
    "Q6sltBxIy_1W",
    "Z0T8GtZknMFg",
    "Aw5V95Tzy_13",
    "mLzsl3Jey_1_",
    "6iIFZg73oBx4",
    "mF9ixuo-naJs",
    "iqEX2HyNndAl"
   ],
   "name": "prototyping.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
